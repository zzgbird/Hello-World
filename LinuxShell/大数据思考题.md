# 思考题

## 1 Spark RDD 每一步骤计算之间的依赖关系有哪几种？

A： Spark 支持两种依赖关系：窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）。

窄依赖就是父 RDD 的分区可以一一对应到子 RDD 的分区，宽依赖就是父 RDD 的每个分区可以被多个子 RDD 的分区使用。

Q: 窄依赖是指父 RDD 的每一个分区都可以唯一对应子 RDD 中的分区，那么是否意味着子 RDD 中的一个分区只可以对应父 RDD 中的一个分区呢？如果子 RDD 的一个分区需要由父 RDD 中若干个分区计算得来，是否还算窄依赖？

A： 窄依赖，就是只有父RDD的分区被多个子RDD的分区利用的时候才是宽依赖



## 2 RDD 的结构

<img src="https://static001.geekbang.org/resource/image/8c/1c/8cae25f4d16a34be77fd3e84133d6a1c.png?wh=1776*2263" alt="img" style="zoom: 25%;" />

**Checkpoint**: 

如果一个 RDD 的依赖链比较长，而且中间又有多个 RDD 出现故障的话，进行恢复可能会非常耗费时间和计算资源。而检查点（Checkpoint）的引入，就是为了优化这些情况下的数据恢复。

存储级别（**Storage Level**）是一个枚举类型，用来记录 RDD 持久化时的存储级别，常用的有以下几个：

MEMORY_ONLY：只缓存在内存中，如果内存空间不够则不缓存多出来的部分。这是 RDD 存储级别的默认值。MEMORY_AND_DISK：缓存在内存中，如果空间不够则缓存在硬盘中。DISK_ONLY：只缓存在硬盘中。MEMORY_ONLY_2 和 MEMORY_AND_DISK_2 等：与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。



**RDD 的操作**

RDD 的数据操作分为两种：转换（Transformation）和动作（Action）。

Transformation: map、filter、groupByKey

Action: Collect、Reduce、Count、CountByKey



Spark 的 persist() 和 cache() 方法支持将 RDD 的数据缓存至内存或硬盘中，

## 3 对 RDD 进行持久化操作和记录 Checkpoint，有什么区别呢？

区别在于Checkpoint会清空该RDD的依赖关系，并新建一个CheckpointRDD依赖关系，让该RDD依赖，并保存在磁盘或HDFS文件系统中，当数据恢复时，可通过CheckpointRDD读取RDD进行数据计算；持久化RDD会保存依赖关系和计算结果至内存中，可用于后续计算。



## 4  Spark SQL

DataFrame 和 DataSet 是 Spark SQL 提供的基于 RDD 的结构化数据抽象。

什么场景适合使用 DataFrame API，什么场景适合使用 DataSet API？

在 Spark 2.0 中，DataFrame 和 DataSet 被统一。DataFrame 作为 DataSet[Row]存在。



## 5 Structured Streaming

与 Spark Streaming 类似，Structured Streaming 也是将输入的数据流按照时间间隔（以一秒为例）划分成数据段。每一秒都会把新输入的数据添加到表中，Spark 也会每秒更新输出结果。

<img src="https://static001.geekbang.org/resource/image/bb/37/bb1845be9f34ef7d232a509f90ae0337.jpg?wh=2152*1262" alt="img" style="zoom: 33%;" />



**Structured Streaming 与 Spark Streaming 对比:**

**A 简易度和性能：**

Spark Streaming 提供的 DStream API 与 RDD API 很类似，相对比较低 level。

Structured Streaming 提供的 DataFrame API 就是这么一个相对高 level 的 API，大部分开发者都很熟悉关系型数据库和 SQL。**这样的数据抽象可以让他们用一套统一的方案去处理批处理和流处理**，不用去关心具体的执行细节。

DataFrame查询语法：

```java
df = … // 这个DataFrame代表学校学生的数据流，schema是{name: string, age: number, height: number, grade: string}
df.select("name").where("age > 10") // 返回年龄大于10岁的学生名字列表
df.groupBy("grade").count() // 返回每个年级学生的人数
df.sort_values([‘age’], ascending=False).head(100) //返回100个年龄最大的学生 
```



**B 实时性:**

Spark Streaming 最小1s

Structured Streaming 最小100毫秒， Spark2.3版本中，Structured Streaming 引入了连续处理的模式，可以做到真正的毫秒级延迟。



**C 对事件时间的支持:**

Structured Streaming 对基于事件时间的处理有很好的支持。

由于 Spark Streaming 是把数据按接收到的时间切分成一个个 RDD 来进行批处理，所以它很难基于数据本身的产生时间来进行处理。



Structured Streaming 还有很多其他优点。比如，它有更好的容错性，保证了端到端 exactly once 的语义等等。所以综合来说，Structured Streaming 是比 Spark Streaming 更好的流处理工具。

**exactly once**

流处理（streaming process），有时也被称为事件处理（event processing），可以被简洁地描述为对于一个无限的数据或事件序列的连续处理。

一个执行流/事件处理应用的流处理引擎通常允许用户制定一个可靠性模式或者处理语义，来标示引擎会为应用图的实体之间的数据处理提供什么样的保证。由于你总是会遇到网络、机器这些会导致数据丢失的故障，因而这些保证是有意义的。有三种模型/标签，at-most-once、at-least-once以及exactly-once，通常被用来描述流处理引擎应该为应用提供的数据处理语义。

**基于 DataFrame API 的 Word Count 程序, pyspark** 

```python

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

if __name__ == "__main__":
   spark = SparkSession
       .builder
       .appName(‘WordCount’)
       .getOrCreate()
   lines = spark.read.text("sample.txt")
   wordCounts = lines
       .select(explode(split(lines.value, " "))
       .alias("word"))
       .groupBy("word")
       .count()
   wordCounts.show()
   
   spark.stop()
```





